#! /usr/bin/env python
#
#  GOAL
#    Simple pipeline: just ILE+fitting iteration jobs
#    User must provide arglists for both ILE and fitting/iteration jobs
#    Assumes user has done all necessary setup (e.g., PSDs, data selection, picking channel names, etc)
#    Will add in fit assessment jobs later
#
#  WHY DO THIS?
#    We're often reanalyzing a single stretch of data again and again. (Or otherwise have some simple setup).
#    This saves time for large-scale injection studies or systematics reruns
#
#    Intend to replace naive iteration code with reproducible, extendable/scalable workflow
#
#  SEE ALSO
#     - original shell-script-based generation scripts
#     - create_event_parameter_pipeline  # long term plan
#     - create_postprocessing_event_dag.py
#     - create_event_dag_via_grid   # basic iteration code
#
# WORKFLOW STRUCTURE
#    - Required elements:
#          cache file  (specified in  --cache <cache> in cip_args)
#          PSD files (.... --psd  in cip_args)
#          channel names
#          event time
#    - Inputs
#         args_cip.txt   # no --fname, --fname-output-...  (overrridden)
#         args_ile.txt    # no --sim-xml,  --event (overrridden)      ... though you may want to add them for command-single.sh to run properly
#    - Workspaces
#          Creates one directory for each task, for each iteration
#          Main directory:
#              - output-ILE-<iteration>.xml.gz, starting with iteration 0 (=input)
#              - *.composite files, from each consolidation step
#              - all.net : union of all *.composite files 
#   
# EXAMPLES
#    python create_event_parameter_pipeline_BasicIteration.py --ile-args args_ile.txt --cip-args args.txt 
#
# TEST SEQUENCE
#        unixhome/Projects/LIGO-ILE-Applications/communications/20180806-Me-PipelineDevelopment
#    * Trivial iteration, fitting a constant likelihood at each step: 
#           rm -rf test_workflow_trivial; make test_workflow_trivial
#         


###
###  PLANS FOR IMPROVEMENT
###
#
# * Stress-testing final fit
#       - single-iteration runs which change lnL and/or coordinates
#       - note this can be done with a DRIVER, does not need to be hardcoded?
# * BCR/BCI (single-ifo vs multi-ifo run)
#       - runs which do PE on single IFOs and 
#       - again, this can be done with a DRIVER

import argparse
import sys
import os
import shutil
import numpy as np
import lalsimutils
import lalsimulation as lalsim
import lal
import functools
import itertools

from glue import pipeline # https://github.com/lscsoft/lalsuite-archive/blob/5a47239a877032e93b1ca34445640360d6c3c990/glue/glue/pipeline.py

import dag_utils
from dag_utils import mkdir
from dag_utils import which

parser = argparse.ArgumentParser()
parser.add_argument("--working-directory",default="./")
parser.add_argument("--input-grid",default="overlap-grid.xml.gz")
parser.add_argument("--cip-args",default=None,help="filename of args_cip.txt file  which holds CIP arguments.  Should NOT conflict with arguments auto-set by this DAG ... in particular, i/o arguments will be modified. ")
parser.add_argument("--cip-args-list",default=None,help="filename of args_cip_list.file, which holds CIP arguments.  Overrides cip-args if present. One CIP_n.sub file is created for each line in the file, which is used for an integer m iterations, where m is the first item of each line (normally 'X' in CIP)")
parser.add_argument("--puff-exe",default=None,help="util_ParameterPuffball")
parser.add_argument("--puff-args",default=None,help="util_ParameterPuffball arguments.  If not specified, puffball will not be performed  ")
parser.add_argument("--puff-cadence",default=None,type=int,help="Every n iterations (not including 0), the puffball code will be applied.  Puffball points will be done *in addition* to the usual results from the DAG.  (The puffball is based on perturbing points from that iteration, and this will roughly double that iteration in ILE job size).  Proposed value 2 (i.e., puff overlap-grid-2, ...-4, ...-6.  If not specified, puffball will not be performed ")
parser.add_argument("--puff-max-it",default=-1,type=int,help="Maximum iteration number that puffball is applied.  If negative, puffball is not applied ")
parser.add_argument("--last-iteration-extrinsic",action='store_true',help="Configure last iteration to extract *one* set of extrinsic parameters from each intrinsic point. [This is highly inefficient, but people like having one extrinsic point per intrinsic point.]  Requires --convert-args")
parser.add_argument("--ile-args",default=None,help="filename of args_ile.txt file  which holds ILE arguments.  Should NOT conflict with arguments auto-set by this DAG ... in particular, i/o arguments will be modified")
parser.add_argument("--ile-exe",default=None,help="filename of ILE or equivalent executable. Will default to `which integrate_likelihood_extrinsic` in low-level code")
parser.add_argument("--ile-retries",default=0,type=int,help="Number of retry attempts for ILE jobs. (These can fail)")
parser.add_argument("--ile-n-events-to-analyze",default=1,type=int,help="If >1, you are using ILE_batchmode.  Structures the DAG correctly to account for batch cadence")
parser.add_argument("--cip-exe",default=None,help="filename of CIP or equivalent executable. Will default to `which util_ConstructIntrinsicPosterior_GenericCoordinates` in low-level code")
parser.add_argument("--test-exe",default=None,help="filename of test code or equivalent executable.  Must have a --test-output <fname> argument.  Used for convergence testing or other termination. NOT ACTIVE; see 'convergence_test_samples.py' for example")
parser.add_argument("--plot-exe",default=None,help="filename of plot code or equivalent executable.  Will default to `which plot_posterior_corner.py`.  Default is to plot last set of samples")
parser.add_argument("--gridinit-exe",default=None,help="filename of initial grid creation or equivalent executable.  Will default to `which util_ManualOverlapGrid.py`.  Must create overlap-grid-0.xml.gz")
parser.add_argument("--test-args",default=None,help="filename of args_test.txt, which holds test arguments.  Note i/o arguments will be modified, so should NOT specify the samples files or the output file, just the test to be performed and any related arguments")
parser.add_argument("--convert-args",default=None,help="filename of args_convert.txt, which holds arguments to the convert function. Needed if you plan to export tide informaton")
parser.add_argument("--plot-args",default=None,help="filename of args_plot.txt, which holds plot arguments.  Note i/o arguments will be modified, so should NOT specify the samples files or the output file, just the test to be performed and any related arguments.  To create the posterior samples file, you probably need to run the convert job (e.g., via the tests)")
parser.add_argument("--gridinit-args",default=None,help="arguments for grid creation or equivalent executable.  If empty, will attempt to use the --input-grid argument")
parser.add_argument("--ile-batch",action='store_true',help="Different workflow: the ILE job is assumed to create a .composite file directly in the current working directory. Designed for fake ILE sets and long-term batch workflow. NOT IMPLEMENTED ")
parser.add_argument("--transfer-file-list",default=None,help="File containing list of *input* filenames to transfer, one name per file. Copied into transfer_files for condor directly.  If provided, also enables attempts to deduce files that need to be transferred for the pipeline to operate, as needed for OSG, etc")
parser.add_argument("--request-memory-ILE",default=4096,type=int,help="Memory request for condor (in Mb) for ILE jobs.")
parser.add_argument("--request-gpu-ILE",action='store_true',help="ILE will request a GPU. [Note: if you do this, your code will only run on GPU-enabled slots]")
parser.add_argument("--request-memory-CIP",default=16384,type=int,help="Memory request for condor (in Mb) for fitting jobs.")
parser.add_argument("--use-singularity",action='store_true',help="Attempts to use a singularity image in SINGULARITY_RIFT_IMAGE")
parser.add_argument("--use-osg",action='store_true',help="Attempts to set up an OSG workflow.  Must submit from an osg allowed submit machine")
parser.add_argument("--frames-dir",default=None,help="If you are using synthetic dat")
parser.add_argument("--cache-file",default=None,help="If you are using real data and have CVMS frames, you want to transfer the cache file over.")
parser.add_argument('--n-copies',default=2,type=int,help="Number of duplicates of each ILE job, for redundant Monte Carlo")
parser.add_argument('--n-iterations',default=3,type=int,help="Number of iterations to perform")
parser.add_argument("--start-iteration",default=0,type=int,help="starting iteration. If >0, does not copy over --input-grid. DOES rewrite sub files.  This allows you to change the arguments provided (e.g., use more iterations or settings at late times). Note this overwrites the .sub files ")
parser.add_argument('--n-samples-per-job',default=2000,type=int,help="Number of samples generated each iteration; also, number of ILE jobs run each iteration. Should increase with dimension of problem")
parser.add_argument('--neff-threshold',default=800,type=int,help="Number of samples generated each iteration")
parser.add_argument("--workflow",default='single',help="[single|fit+posterior|full] describes workflow layout used.  'Single' is a single node, running the fit and posterior for each iteration; 'full' produces many followup jobs to produce a reliable posterior")
parser.add_argument("--n-post-jobs",default=1,type=int,help="Number of posterior jobs. Used in posterior and fit+posterior workflows")
opts=  parser.parse_args()

working_dir_inside = opts.working_dir  
if opts.use_singularity:
    working_dir_inside = "./" # all files on the remote machine are in the current directory

singularity_image = None
if opts.use_singularity:
    print " === USING SINGULARITY === "
    singularity_image = os.environ["SINGULARITY_RIFT_IMAGE"]  # must be present to use singularity
    print singularity_image

if (opts.cip_args is None) and (opts.cip_args_list is None):
    print " No arguments provided for low-level job"
    sys.exit(0)

# Load args.txt. Remove first item.  Store
if not (opts.cip_args is None):
    with open(opts.cip_args) as f:
        cip_args_list = f.readlines()
    cip_args = ' '.join( map( lambda x: x.replace('\\',''),cip_args_list) )
    cip_args = ' '.join(cip_args.split(' ')[1:])
    # Some argument protection for later
    cip_args = cip_args.replace('[', ' \'[')
    cip_args = cip_args.replace(']', ']\'')
    cip_args=cip_args.rstrip()
    cip_args += ' --no-plots '  
    print "CIP", cip_args

cip_args_lines = None
if not (opts.cip_args_list is None):
    with open(opts.cip_args_list) as f:
        cip_args_lines = f.readlines()
    cip_args_n = map(lambda x: int(x.split(' ')[0]), cip_args_lines)  # Pull off the integer
    cip_args_lines = map(lambda x: ' '.join(x.split(' ')[1:]), cip_args_lines) # pull off the integer
    cip_args_lines = map( lambda x : x.replace('[', ' \'[').replace(']', ']\'').rstrip(), cip_args_lines)
    cip_args_lines = map( lambda x: x.lstrip(), cip_args_lines ) # remove leading whitespace
    for line in cip_args_lines:
        print " CIP ", line

    if np.sum(cip_args_n) < opts.n_iterations:
        print " Not enough CIP versions to complete all requested iterations; extending "
        cip_args_n[-1]  = opts.n_iterations - np.sum(cip_args_n)

# Load args.txt. Remove first item.  Store
with open(opts.ile_args) as f:
    ile_args_list = f.readlines()
ile_args = ' '.join( map( lambda x: x.replace('\\',''),ile_args_list) )
ile_args = ' '.join(ile_args.split(' ')[1:])
# Some argument protection for later
ile_args = ile_args.replace('[', ' \'[')
ile_args = ile_args.replace(']', ']\'')
ile_args=ile_args.rstrip()
if opts.request_gpu_ILE:
    ile_args+=" --vectorized --gpu "  # append this
if opts.ile_n_events_to_analyze > 1:
    ile_args+= " --n-events-to-analyze " + str(opts.ile_n_events_to_analyze)
#ile_args += ' --soft-fail-event-range '  #important in case event is out of range. Simply exit NO ...not backwards compatible
print "ILE", ile_args

puff_args=None
puff_cadence = None
puff_max_it = opts.puff_max_it
if opts.puff_args and opts.puff_cadence:
    puff_cadence = opts.puff_cadence
    # Load args.txt. Remove first item.  Store
    with open(opts.puff_args) as f:
        puff_args_list = f.readlines()
    puff_args = ' '.join( map( lambda x: x.replace('\\',''),puff_args_list) )
    puff_args = ' '.join(puff_args.split(' ')[1:])
# Some argument protection for later
    puff_args = puff_args.replace('[', ' \'[')
    puff_args = puff_args.replace(']', ']\'')
    puff_args=puff_args.rstrip()
    print "PUFF", puff_args
    print "PUFF CADENCE", puff_cadence


test_args=None
if opts.test_args:
    # Load args.txt. Remove first item.  Store
    with open(opts.test_args) as f:
        test_args_list = f.readlines()
    test_args = ' '.join( map( lambda x: x.replace('\\',''),test_args_list) )
    test_args = ' '.join(test_args.split(' ')[1:])
# Some argument protection for later
    test_args = test_args.replace('[', ' \'[')
    test_args = test_args.replace(']', ']\'')
    test_args=test_args.rstrip()
    print "CONVERGE", test_args

plot_args=None
if opts.plot_args:
    # Load args.txt. Remove first item.  Store
    with open(opts.plot_args) as f:
        plot_args_list = f.readlines()
    plot_args = ' '.join( map( lambda x: x.replace('\\',''),plot_args_list) )
    plot_args = ' '.join(plot_args.split(' ')[1:])
# Some argument protection for later
    plot_args = plot_args.replace('[', ' \'[')
    plot_args = plot_args.replace(']', ']\'')
    plot_args=plot_args.rstrip()
    print "PLOT", plot_args

convert_args=None
if opts.convert_args:
    # Load args.txt. Remove first item.  Store
    with open(opts.convert_args) as f:
        convert_args_list = f.readlines()
    convert_args = ' '.join( map( lambda x: x.replace('\\',''),convert_args_list) )
    convert_args = ' '.join(convert_args.split(' ')[1:])
# Some argument protection for later
    convert_args = convert_args.replace('[', ' \'[')
    convert_args = convert_args.replace(']', ']\'')
    convert_args=convert_args.rstrip()
    print "CONVERT", convert_args

gridinit_args=None
if opts.gridinit_args:
    # Load args.txt. Remove first item.  Store
    with open(opts.gridinit_args) as f:
        gridinit_args_list = f.readlines()
    gridinit_args = ' '.join( map( lambda x: x.replace('\\',''),gridinit_args_list) )
    gridinit_args = ' '.join(gridinit_args.split(' ')[1:])
# Some argument protection for later
    gridinit_args = gridinit_args.replace('[', ' \'[')
    gridinit_args = gridinit_args.replace(']', ']\'')
    gridinit_args=gridinit_args.rstrip()
    print "GRIDINIT", gridinit_args


# Copy seed grid into place as overlap-grid-0.xml.gz
it_start = opts.start_iteration
n_initial = opts.n_samples_per_job
if (it_start is 0) and not gridinit_args:
    shutil.copyfile(opts.input_grid,"overlap-grid-0.xml.gz")  # put in working directory !
    n_initial = len(lalsimutils.xml_to_ChooseWaveformParams_array("overlap-grid-0.xml.gz"))

transfer_file_names = None
if not (opts.transfer_file_list is None):
    transfer_file_names=[]
    # Load args.txt. Remove first item.  Store
    with open(opts.transfer_file_list) as f:
        for  line in f.readlines():
            transfer_file_names.append(line.rstrip())
    print " Input files to transfer to job working directory (note!)", transfer_file_names


###
### Fiducial fit job (=sanity check that code will run)
###
if (opts.cip_args is None):
    cip_args = cip_args_lines[0]
cmdname="%s/command-single_fit.sh" % opts.working_directory
cmd = open(cmdname, 'w')
arg_list = cip_args
exe = dag_utils.which("util_ConstructIntrinsicPosterior_GenericCoordinates.py")
cmd.write('#!/usr/bin/env bash\n')
cmd.close()
st = os.stat(cmdname)
import stat
os.chmod(cmdname, st.st_mode | stat.S_IEXEC)


cmdname="%s/command-single.sh" % opts.working_directory
cmd = open(cmdname, 'w')
arg_list = ile_args
exe = dag_utils.which("integrate_likelihood_extrinsic")
if opts.ile_n_events_to_analyze:
    exe = dag_utils.which("integrate_likelihood_extrinsic_batchmode")
cmd.write('#!/usr/bin/env bash\n')
cmd.write('# (run me only in working directories)\n')
cmd.write(exe + ' ' + arg_list + " --sim-xml overlap-grid-0.xml.gz")  # make it concrete, so I can test it
cmd.close()
st = os.stat(cmdname)
import stat
os.chmod(cmdname, st.st_mode | stat.S_IEXEC)
# Add argument to ile_args.txt to 
#    identify overlap grid input
#    identify output file names (?)
ile_args_orig = ile_args  # provides ability to strip out the output and replace it with alternate
ile_args+= ' --sim-xml ' + working_dir_inside + '/overlap-grid-$(macroiteration).xml.gz '
ile_args_forpuff= ile_args_orig + ' --sim-xml ' + working_dir_inside + '/puffball-$(macroiteration).xml.gz '


###
### DAG generation
###

dag = pipeline.CondorDAG(log=os.getcwd())


# Make directories for all iterations
for indx in np.arange(it_start,opts.n_iterations):
    ile_dir = opts.working_directory+"/iteration_"+str(indx)+"_ile"
    cip_dir = opts.working_directory+"/iteration_"+str(indx)+"_cip"
    consolidate_dir = opts.working_directory+"/iteration_"+str(indx)+"_con"
#    convert_dir = opts.working_directory+"/iteration_"+str(indx)+"_change"
    mkdir(ile_dir); mkdir(ile_dir+"/logs")
    mkdir(cip_dir);  mkdir(cip_dir+"/logs")
    mkdir(consolidate_dir); mkdir(consolidate_dir+"/logs")
#    mkdir(change_dir); mkdir(change_dir+"/logs")

    if opts.test_args:
        test_dir = opts.working_directory+"/iteration_"+str(indx)+"_test"
        mkdir(test_dir); mkdir(test_dir+'/logs')
        
    if opts.plot_args:  # Overkill: currently only making plots on last iteration
        plot_dir = opts.working_directory+"/iteration_"+str(indx)+"_plot"
        mkdir(plot_dir); mkdir(plot_dir+'/logs')



# ++++
# Create workflow tasks
# ++++

##   ILE job

ile_exe =opts.ile_exe
if (opts.ile_n_events_to_analyze > 1) and (exe is None):
    ile_exe = dag_utils.which("integrate_likelihood_extrinsic_batchmode")
output_file_names = None
if opts.use_singularity:
    output_file_names = ','.join(["CME_out-$(macroevent)-$(cluster)-$(process)-{0}.dat".format(x) for x in np.arange(opts.ile_n_events_to_analyze)])
    print "OUTPUT FILES ", output_file_names
ile_job, ile_job_name = dag_utils.write_ILE_sub_simple(tag='ILE',log_dir=None,arg_str=ile_args,output_file="CME_out.xml",ncopies=opts.n_copies,exe=ile_exe,transfer_files=transfer_file_names,transfer_output_files=output_file_names,request_memory=opts.request_memory_ILE,request_gpu=opts.request_gpu_ILE,use_singularity=opts.use_singularity,singularity_image=singularity_image,use_osg=opts.use_osg,frames_dir=opts.frames_dir,cache_file=opts.cache_file)
# Modify: create macro for iteration 
#   - added on a per-node basis
# Modify: add macro argument for overlap grid to be used (kept in top-level directory)
# Modify: set 'initialdir'
#     NOTE: logs will be specified relative to this directory
ile_job.add_condor_cmd("initialdir",opts.working_directory+"/iteration_$(macroiteration)_ile")
# Modify output argument: change logs and working directory to be subdirectory for the run
ile_job.set_log_file(opts.working_directory+"/iteration_$(macroiteration)_ile/logs/ILE-$(macroevent)-$(cluster)-$(process).log")
ile_job.set_stderr_file(opts.working_directory+"/iteration_$(macroiteration)_ile/logs/ILE-$(macroevent)-$(cluster)-$(process).err")
ile_job.set_stdout_file(opts.working_directory+"/iteration_$(macroiteration)_ile/logs/ILE-$(macroevent)-$(cluster)-$(process).out")
ile_job.write_sub_file()

if not (opts.puff_args is None):
    # Write a version of the ILE job that uses puffball inputs ... IDENTICAL to code above for standard ILE case,just different argument for input
    # Yes, it would logically be simpler to make overlap-grid.xml.gz larger ... but I want to keep 'real samples' and 'puffed samples' seperate.
    ilePuff_job, ilePuff_job_name = dag_utils.write_ILE_sub_simple(tag='ILE_puff',log_dir=None,arg_str=ile_args_forpuff,output_file="CME_out.xml",ncopies=opts.n_copies,exe=ile_exe,transfer_files=transfer_file_names,request_memory=opts.request_memory_ILE,request_gpu=opts.request_gpu_ILE)
    ilePuff_job.add_condor_cmd("initialdir",opts.working_directory+"/iteration_$(macroiteration)_ile")
    ilePuff_job.set_log_file(opts.working_directory+"/iteration_$(macroiteration)_ile/logs/ILE-$(macroevent)-$(cluster)-$(process).log")
    ilePuff_job.set_stderr_file(opts.working_directory+"/iteration_$(macroiteration)_ile/logs/ILE-$(macroevent)-$(cluster)-$(process).err")
    ilePuff_job.set_stdout_file(opts.working_directory+"/iteration_$(macroiteration)_ile/logs/ILE-$(macroevent)-$(cluster)-$(process).out")
    ilePuff_job.write_sub_file()


##   Consolidate job(s)
#   - consolidate output of single previous job
con_job, con_job_name = dag_utils.write_consolidate_sub_simple(tag='join',log_dir=None,arg_str='',base=opts.working_directory+"/iteration_$(macroiteration)_ile", target=opts.working_directory+'/consolidated_$(macroiteration)')
# Modify: set 'initialdir' : should run in top-level direcory
#con_job.add_condor_cmd("initialdir",opts.working_directory+"/iteration_$(macroiteration)_con")
# Modify output argument: change logs and working directory to be subdirectory for the run
con_job.set_log_file(opts.working_directory+"/iteration_$(macroiteration)_con/logs/con-$(cluster)-$(process).log")
con_job.set_stderr_file(opts.working_directory+"/iteration_$(macroiteration)_con/logs/con-$(cluster)-$(process).err")
con_job.set_stdout_file(opts.working_directory+"/iteration_$(macroiteration)_con/logs/con-$(cluster)-$(process).out")
con_job.write_sub_file()

##   Unify job
#   - update 'all.net' to include all previous events
unify_job, unify_job_name = dag_utils.write_unify_sub_simple(tag='unify',log_dir='',arg_str='', base=opts.working_directory, target=opts.working_directory+'/all.net')
unify_job.add_condor_cmd("initialdir",opts.working_directory)
unify_job.set_log_file(opts.working_directory+"/iteration_$(macroiteration)_con/logs/unify-$(cluster)-$(process).log")
unify_job.set_stderr_file(opts.working_directory+"/iteration_$(macroiteration)_con/logs/unify-$(cluster)-$(process).err")
unify_job.set_stdout_file(opts.working_directory+"/all.net")
unify_job.write_sub_file()




##   Fit job: default case
cip_job, cip_job_name = dag_utils.write_CIP_sub(tag='CIP',log_dir=None,arg_str=cip_args,request_memory=opts.request_memory_CIP,input_net=opts.working_directory+'/all.net',output='overlap-grid-$(macroiterationnext)',out_dir=opts.working_directory,exe=opts.cip_exe)
# Modify: set 'initialdir'
cip_job.add_condor_cmd("initialdir",opts.working_directory+"/iteration_$(macroiteration)_cip")
# Modify output argument: change logs and working directory to be subdirectory for the run
cip_job.set_log_file(opts.working_directory+"/iteration_$(macroiteration)_cip/logs/cip-$(cluster)-$(process).log")
cip_job.set_stderr_file(opts.working_directory+"/iteration_$(macroiteration)_cip/logs/cip-$(cluster)-$(process).err")
cip_job.set_stdout_file(opts.working_directory+"/iteration_$(macroiteration)_cip/logs/cip-$(cluster)-$(process).out")
cip_job.write_sub_file()


##   puffball job: default case
if puff_args and puff_cadence:
    puff_job, puff_job_name = dag_utils.write_puff_sub(tag='PUFF',log_dir=None,arg_str=puff_args,request_memory=opts.request_memory_ILE,input_net=opts.working_directory+'/overlap-grid-$(macroiterationnext).xml.gz',output=opts.working_directory+'/puffball-$(macroiterationnext)',out_dir=opts.working_directory,exe=opts.puff_exe)
    # Modify: set 'initialdir' to CIP WORKING DIR 
    puff_job.add_condor_cmd("initialdir",opts.working_directory+"/iteration_$(macroiteration)_cip")
    # Modify output argument: change logs and working directory to be subdirectory for the run
    puff_job.set_log_file(opts.working_directory+"/iteration_$(macroiteration)_cip/logs/puff-$(cluster)-$(process).log")
    puff_job.set_stderr_file(opts.working_directory+"/iteration_$(macroiteration)_cip/logs/puff-$(cluster)-$(process).err")
    puff_job.set_stdout_file(opts.working_directory+"/iteration_$(macroiteration)_cip/logs/puff-$(cluster)-$(process).out")
    puff_job.write_sub_file()



cip_job_list = None
if  (cip_args_lines is None):
    # Write the default cip_job into cip_job_list n times
    cip_job_list =opts.n_iterations* [cip_job ]
else:  
    # we have different cip jobs for different iteration numbers
    cip_job_list = []
    for indx in np.arange(len(cip_args_lines)):
       # Write the appropriate CIP jobs. [note only one CIP per iteration, so unique
       cip_job, cip_job_name = dag_utils.write_CIP_sub(tag='CIP_'+str(indx),log_dir=None,arg_str=cip_args_lines[indx],request_memory=opts.request_memory_CIP,input_net=opts.working_directory+'/all.net',output='overlap-grid-$(macroiterationnext)',out_dir=opts.working_directory,exe=opts.cip_exe)
        # Modify: set 'initialdir'
       cip_job.add_condor_cmd("initialdir",opts.working_directory+"/iteration_$(macroiteration)_cip")
        # Modify output argument: change logs and working directory to be subdirectory for the run
       cip_job.set_log_file(opts.working_directory+"/iteration_$(macroiteration)_cip/logs/cip-$(cluster)-$(process).log")
       cip_job.set_stderr_file(opts.working_directory+"/iteration_$(macroiteration)_cip/logs/cip-$(cluster)-$(process).err")
       cip_job.set_stdout_file(opts.working_directory+"/iteration_$(macroiteration)_cip/logs/cip-$(cluster)-$(process).out")
       cip_job.write_sub_file()

       # augment the CIP job list
       cip_job_list = cip_job_list + cip_args_n[indx]*[cip_job]

##   Test job (terminate, convergence
if opts.test_args:
    ##  Convert job : make results accessible after every iteration.  (Only performed if the tests are active, to make my life easier)
    convert_job, convert_job_name =dag_utils.write_convert_sub(tag='convert',log_dir=None,arg_str=convert_args,file_input=opts.working_directory+'/overlap-grid-$(macroiteration).xml.gz', file_output=opts.working_directory+'/posterior_samples-$(macroiteration).dat' ,out_dir=opts.working_directory,exe=opts.test_exe)
    convert_job.add_condor_cmd("initialdir",opts.working_directory)
    convert_job.set_log_file(opts.working_directory+"/iteration_$(macroiterationlast)_test/logs/convert-$(cluster)-$(process).log")
    convert_job.set_stderr_file(opts.working_directory+"/iteration_$(macroiterationlast)_test/logs/convert-$(cluster)-$(process).err")
    convert_job.write_sub_file()


    test_job, test_job_name = dag_utils.write_test_sub(tag='test',log_dir=None,arg_str=test_args,samples_files=[ opts.working_directory+'/posterior_samples-$(macroiteration).dat', opts.working_directory+'/posterior_samples-$(macroiterationlast).dat'] ,out_dir=opts.working_directory,exe=opts.test_exe)
    # Modify: set 'initialdir'
    test_job.add_condor_cmd("initialdir",opts.working_directory+"/iteration_$(macroiteration)_test")
    # Modify output argument: change logs and working directory to be subdirectory for the run
    test_job.set_log_file(opts.working_directory+"/iteration_$(macroiteration)_test/logs/test-$(cluster)-$(process).log")
    test_job.set_stderr_file(opts.working_directory+"/iteration_$(macroiteration)_test/logs/test-$(cluster)-$(process).err")
    test_job.set_stdout_file(opts.working_directory+"/iteration_$(macroiteration)_test/logs/test-$(cluster)-$(process).out")
    test_job.write_sub_file()


if opts.plot_args and opts.test_args:
    # default: last two iterations
    plot_job, plot_job_name = dag_utils.write_plot_sub(tag='plot',log_dir=None,arg_str=plot_args,samples_files=['posterior_samples-$(macroiteration).dat','posterior_samples-$(macroiterationlast).dat'] ,out_dir=opts.working_directory,exe=opts.plot_exe)
    plot_job.set_log_file(opts.working_directory+"/iteration_$(macroiteration)_plot/logs/test-$(cluster)-$(process).log")
    plot_job.set_stderr_file(opts.working_directory+"/iteration_$(macroiteration)_plot/logs/test-$(cluster)-$(process).err")
    plot_job.set_stdout_file(opts.working_directory+"/iteration_$(macroiteration)_plot/logs/test-$(cluster)-$(process).out")
    plot_job.write_sub_file()


if opts.gridinit_args:
    gridinit_job, gridinit_job_name = dag_utils.write_init_sub(tag='grid',log_dir=None,arg_str=gridinit_args,out_dir=opts.working_directory,exe=opts.gridinit_exe)
    gridinit_job.set_log_file(opts.working_directory+"/init-$(cluster)-$(process).log")
    gridinit_job.set_stderr_file(opts.working_directory+"init--$(cluster)-$(process).err")
    gridinit_job.set_stdout_file(opts.working_directory+"/init-$(cluster)-$(process).out")
    gridinit_job.write_sub_file()


##   Convert job(s)
#    - relocate input grid to correct location.  (MAKE TRIVIAL: do via output arguments in previous stage!)
##   Assessment job (no-op for now)




# ++++
# Create workflow 
# ++++

# Create workflow
#   - Create grid node as needed
#   - Loop over iterations
#      - if iteration0, use seed grid (should already be copied in place)
#      - if not iteration 0, grid should be in place (from previous stage)
#      - Loop over events, make ILE node per event
#      - create consolidate job, make it depend on all events in that iteration
#      - create fit job, make it depend on consolidate job
#    

parent_fit_node = None

if opts.gridinit_args:
   grid_node = pipeline.CondorDAGNode(gridinit_job) 
   dag.add_node(grid_node)
   parent_fit_node = grid_node  # this must happen before the first ILE jobs


for it in np.arange(it_start,opts.n_iterations):
    consolidate_now = None
    fit_node_now = None
    ile_nodes_now =[]
    # Create consolidate job
    con_node = pipeline.CondorDAGNode(con_job)
    con_node.add_macro("macroiteration",it)
    # Create unify job
    unify_node = pipeline.CondorDAGNode(unify_job)
    unify_node.add_macro("macroiteration",it)
    unify_node.add_parent(con_node)
    
    # Create one node per job
    n_jobs_this_time = opts.n_samples_per_job
    if it ==it_start:
        n_jobs_this_time = n_initial
    n_group = opts.ile_n_events_to_analyze
    indx_max = int((1.0*n_jobs_this_time)/n_group)
    if indx_max*n_jobs_this_time < n_group:
        indx_max+=1
    for event in np.arange(indx_max): #np.arange(n_jobs_this_time):
        # Add task per ILE operation
        ile_node = pipeline.CondorDAGNode(ile_job)
#        ile_node.set_priority(JOB_PRIORITIES["ILE"])
        ile_node.set_retry(opts.ile_retries)
        ile_node.add_macro("macroevent", event*n_group)
        ile_node.add_macro("macroiteration", it)
        if not(parent_fit_node is None):
            ile_node.add_parent(parent_fit_node)
        con_node.add_parent(ile_node) # consolidate depends on all of the individual jobs
        dag.add_node(ile_node)

    if puff_args and puff_cadence:
     if it>1 and it <= puff_max_it  and (it-1)%puff_cadence ==0:  # we made a puffball last iteration, so run it through ILE now
        print " ILE jobs for puffball on iteration ", it
        for event in np.arange(indx_max):
            ile_node = pipeline.CondorDAGNode(ilePuff_job)  # only difference is here: uses puffball, which by construction is the same size/ perturbed points
            ile_node.add_macro("macroevent", event*n_group)
            ile_node.add_macro("macroiteration", it)
            if not(parent_fit_node is None):
                ile_node.add_parent(parent_fit_node)
            con_node.add_parent(ile_node) # consolidate depends on all of the individual jobs
            dag.add_node(ile_node)

    # add con job
    dag.add_node(con_node)
    dag.add_node(unify_node)

    # Create fit node, which depends on consolidate node
    cip_job = cip_job_list[it]
    fit_node = pipeline.CondorDAGNode(cip_job)
    fit_node.add_macro("macroiteration", it)
    fit_node.add_macro("macroiterationnext", it+1)
    fit_node.set_category("CIP")
    fit_node.add_parent(unify_node)  # only fit if we have results from the previous iteration
    dag.add_node(fit_node) 

    parent_fit_node = fit_node

    # Check if puffball being created, and if so create it *immediately* after CIP job creates the overlap-grid file
    if puff_args and puff_cadence:
      if it > 0 and it <= puff_max_it and it%puff_cadence ==0:
        print " Puffball for iteration ", it
        puff_node = pipeline.CondorDAGNode(puff_job)
        puff_node.add_macro("macroiteration", it)
        puff_node.add_macro("macroiterationnext", it+1)
        puff_node.set_category("PUFF")
        puff_node.add_parent(parent_fit_node)  # only fit if we have results from the previous iteration
        dag.add_node(puff_node) 
        
        parent_fit_node = puff_node

        

    # Create convert node, which depends on fit node, *if* tests are being performed
    if opts.test_args:
        convert_node=pipeline.CondorDAGNode(convert_job)
        convert_node.add_macro("macroiteration", it+1)  # convert the NEWLY-PRODUCED iteration
        convert_node.add_macro("macroiterationlast", it)  # use log files in the previous directory
        convert_node.add_parent(parent_fit_node)
        convert_node.set_category("CONVERT")
        dag.add_node(convert_node)

        parent_fit_node = convert_node



    if opts.test_args and it>0:
        # Cannot run test on first iteration
        test_node = pipeline.CondorDAGNode(test_job)
        test_node.add_macro("macroiteration", it+1)   # test the NEWLY-PRODUCED iteration against the old
        test_node.add_macro("macroiterationlast", it)
        test_node.add_parent(parent_fit_node)
        test_node.set_category("CONVERGE")
        dag.add_node(test_node)

        parent_fit_node=test_node


# Create export stages for extrinsic samples: 
if opts.last_iteration_extrinsic:
    print "Not implemented yet ... requires information "

# Create final node for overall plots.  (Note: default setup is designed to enable plots of the last two iterations *at each step* but this seems like overkill)
if plot_args:
        # Cannot run test on first iteration
        plot_node = pipeline.CondorDAGNode(plot_job)
        plot_node.add_macro("macroiteration", it)
        plot_node.add_macro("macroiterationlast", it-1)
        plot_node.add_parent(parent_fit_node)
        plot_node.set_category("PLOT")
        dag.add_node(plot_node)

dag_name="marginalize_intrinsic_parameters_BasicIterationWorkflow"
dag.set_dag_file(dag_name)
dag.write_concrete_dag()
